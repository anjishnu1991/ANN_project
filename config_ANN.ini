[General]

#General settings
RunID = 0 # This does nothing, just for output file names
Seed = 193
#InputPath = tW_tt_v29_parton #/cephfs/user/s6niboei/data/processed/1j1b/tW_tt_v29_DS
InputPath = /cephfs/user/s6niboei/data/processed/2j2b/tW_tt_v29_DS # data should be fed in from cephfs 
SignalSample = wt_nominal
BackgroundSample = tt_nominal
SignalSystematicsSample = wt_systematic
#BackgroundSystematicsSample = tt_systematic #set to NONE if systematic does not exist in background sample
BackgroundSystematicsSample = NONE
#Systematic = parton
Systematic = DS # this doesnt actually do anything atm
Region = 2j2b
Verbosity = 0 # verbosity of tensorflow
DebugLevel = INFO # for debugging
UseXLA = True # Should be faster, unless XLA is broken (which it often is)

# Directories
SaveTarFile = False # Whether to output predictions etc. in a tar file
SaveRootFile = False # whether to save a root file including the prediction. Keep in mind this will be ~the size of the input root file
RootFileDir = /cephfs/user/s6niboei/ANN_root/
LogFileDir = /cephfs/user/s6niboei/baf_logs/
ResultsDir = /cephfs/user/s6niboei/ANN_results/

#Discriminator settings
DiscriminatorLayers = 5
DiscriminatorNodes = 128
DiscriminatorEpochs = 10 # Only for pretraining
DiscriminatorDropout = 0.3
DiscriminatorInputDropout = 0.1 # Dropout for the input variables
DiscriminatorOptimizer = SGD # only for pretraining
DiscriminatorActivation = sigmoid
DiscriminatorLearningRate = 0.2 # only for pretraining
DiscriminatorMomentum = 0.8 # Only for SGD optimizer

#Adversary Settings
AdversaryLayers = 4
AdversaryNodes = 128
AdversaryEpochs = 10 # Only for pretraining
AdversaryLearningRate = 0.01 # Only for pretraining
AdversaryMomentum = 0.8
AdversaryDropout = 0.3
AdversaryActivation = sigmoid
AdversaryInitializer = glorot_normal
AdversaryOptimizer = SGD

#Settings for combined training
CombinedLearningRate = 0.5
CombinedMomentum = 0.8 # only for SGD optimizer
CombinedEpochs = 1 # Epochs per iteration (this does nothing atm)
CombinedOptimizer = SGD # available: SGD, AdaGrad, Adam
UseLastLayer = False # Feed the last layer of the classifier into the adversary instead of the output node
BatchNormalization = True

EarlyStoppingCombined = True
EarlyStoppingDiscriminator = True
EarlyStoppingPatience = 10 

TrainingIterations = 10
ValidationFraction = 0.4
LambdaValue = 0.5
TrainAdversarial = True #equivalent to setting lambda to 0
BatchSize = 65536